# ğŸ“Š Mini ETL Pipeline with PySpark

## Overview

This project implements a **mini ETL (Extract, Transform, Load) pipeline** using **PySpark** on application usage / event log data collected over multiple days.

Each record represents a single event generated by a user or device at a specific timestamp.
The pipeline combines raw CSV files, performs data quality checks, cleans the data, runs exploratory analysis, and prepares the dataset for efficient downstream analytics.

---

## ğŸ“ Dataset Description

The dataset spans **5 days** (Jan 10â€“14, 2026) and contains the following fields:

| Column  | Type      | Description                |
| ------- | --------- | -------------------------- |
| date    | date      | Event date                 |
| time    | timestamp | Exact event timestamp      |
| size    | integer   | Payload / download size    |
| version | string    | Application version        |
| os      | string    | Operating system           |
| country | string    | User country               |
| ip_id   | integer   | Anonymized user identifier |

---

## ğŸ› ï¸ Tech Stack

* **Apache Spark (PySpark)**
* **Python**
* Local Spark (`local[*]`)
* CSV input, Parquet output

---

## ğŸ”„ ETL Pipeline Steps

### 1ï¸âƒ£ Extract

* Reads multiple daily CSV files (`2026-01-10` to `2026-01-14`)
* Uses schema inference and header-based parsing

```python
spark.read.option("header", True).option("inferSchema", True).csv(...)
```

---

### 2ï¸âƒ£ Transform

#### Data Combination

* All daily datasets are combined using `union`

#### Caching

* DataFrame is cached to avoid repeated recomputation

#### Data Quality Checks

* **Null check:** No null values found
* **Duplicate check:**

  * 80 duplicate records found out of 154,360
  * Duplicates removed using `distinct()`

#### Schema Validation

* Schema inspected using `printSchema()`
* Physical execution plan analyzed using `explain()`

---

### 3ï¸âƒ£ Load (Optional Output Step)

* Cleaned data is written in **Parquet format**
* Partitioned by:

  * `date`
  * `country`

```python
df_combined.write \
  .mode("overwrite") \
  .partitionBy("date", "country") \
  .parquet("project/output/download_logs")
```

This enables:

* Faster reads
* Partition pruning
* Scalable analytics

---

## ğŸ“ˆ Exploratory Data Analysis

### ğŸ“… Records per Day

* Traffic increases significantly on **Jan 12â€“14**
* Weekday traffic is higher than weekend traffic

### ğŸŒ Requests by Country

* Majority of traffic originates from **US**
* Suggests higher load and importance of US-based infrastructure

### ğŸ’» OS Distribution

* Windows is the dominant operating system
* macOS and source builds contribute smaller portions

### â±ï¸ Hourly Traffic Analysis

* Peak usage between **1 PM and 10 PM**
* Indicates high server load during afternoon and evening hours

### ğŸ§© Version Popularity

* Version **4.5.2** is overwhelmingly the most used
* Bug fixes and optimizations in this version would impact most users

---

## ğŸ“Œ Key Insights

* Data quality is high (no nulls, minimal duplicates)
* Traffic is geographically concentrated
* Clear peak usage windows exist
* One dominant application version drives most usage

---

## ğŸ§  Learnings

* Built a complete ETL pipeline using PySpark
* Performed real-world data quality checks
* Used caching and physical plan inspection for performance awareness
* Applied partitioned storage best practices

---

## ğŸš€ How to Run

1. Start a PySpark environment
2. Place CSV files in `project/data/`
3. Run the Python script
4. (Optional) Enable the Parquet write section for partitioned output

---

## âš ï¸ Notes

* Generated output data (`project/output/`) should **not** be committed to Git
* Use `.gitignore` to exclude large data files

---

## ğŸ“Œ Future Improvements

* Add session-level analysis per user
* Detect anomalous or bot traffic
* Build aggregated daily metrics tables
* Register output as a Spark SQL / Hive table

---
