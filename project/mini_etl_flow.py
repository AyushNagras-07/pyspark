from pyspark.sql import SparkSession
from pyspark.sql import functions as sparkfunc 
from pyspark.sql.functions import col,when,sum
from pyspark.sql.functions import hour


spark = SparkSession.builder.master("local[*]").appName("MiniETL").getOrCreate()

#going to build a etl pipeline 
# the dataset represents application usage / event logs collected over multiple days.
# Each row corresponds to one event generated by a user or device at a specific time.


#reading the data
#for date from 2026-01-10 to 2026-01-14
df = spark.read.option("header", True).option("inferSchema", True).csv("project/data/2026-01-10-r.csv")
df1 = spark.read.option("header", True).option("inferSchema", True).csv("project/data/2026-01-11-r.csv")
df2 = spark.read.option("header", True).option("inferSchema", True).csv("project/data/2026-01-12-r.csv")
df3 = spark.read.option("header", True).option("inferSchema", True).csv("project/data/2026-01-13-r.csv")
df4 = spark.read.option("header", True).option("inferSchema", True).csv("project/data/2026-01-14-r.csv")
# df5 = spark.read.option("header", True).option("inferSchema", True).csv("project/data/2026-01-14.csv")

#combining all dates data together
df_combined = df.union(df2).union(df3).union(df4).union(df1)

df_combined.show()

#storing the result so not to perform the action again and again
# using cache 
df_combined.cache()

#check the number of rows
print(df_combined.count())

#checking the data quality so that data can analysized

# checking duplicate rows
df_combined.exceptAll(df_combined.dropDuplicates()).show()

# checking number of null values
df_combined.select(*(sum(col(c).isNull().cast("int")).alias(c) for c in df_combined.columns)).show()

#there are no null values 
# but some duplicate values
#lets check how much duplicate values are there so that we can take action accordingly

print(df_combined.exceptAll(df_combined.dropDuplicates()).count())
#out of 154360 only 80 records are duplicate so we will consider distinct record and drop duplicate records

df_combined = df_combined.distinct()

#now lets do some analysis
#for that lets first understand dataset

df_combined.explain()
print(df_combined.describe())
df_combined.printSchema()

df_combined.cache()
#counting number of orders/records per day
df_combined.groupBy("date").count().orderBy("date").show()

# +----------+-----+                                                              
# |      date|count|
# +----------+-----+
# |2026-01-10|18620|
# |2026-01-11|20668|
# |2026-01-12|36697|
# |2026-01-13|40732|
# |2026-01-14|37563|
# +----------+-----+

# there is hike of records on 12,13,14 jan 2026 compared to old records
# the traffic in weekends is low and in weekdays is high 

#checking request per country 
df_combined.groupBy("country").count().orderBy("count", ascending=False).show()

# +-------+-----+                                                                 
# |country|count|
# +-------+-----+
# |     US|80365|
# |     NA|20962|
# |     DE| 5841|
# |     CN| 5089|
# |     GB| 5050|
# |     FR| 4775|
# |     CA| 4383|
# |     JP| 2381|
# |     IN| 2060|
# +-------+-----+
#from this we get a lot of clearance that 
# most of the users and requests are from US
# seeing the difference between number 1 and number 2 we can clearly say that servers at us should be handled more carefully

#cheking the os distribution 
df_combined.groupBy("os").count().show()

# as expected most user use windows 
# nothing to interpret from this 

#cheking hourly analysis
df_combined.withColumn("hour", hour("time")).groupBy("hour").count().orderBy("hour").show(n=24, truncate=False)
# +----+-----+                                                                    
# |hour|count|
# +----+-----+
# |   0| 4016|
# |   1| 5383|
# |   2| 4971|
# |   3| 6781|
# |   4| 5674|
# |   5| 5776|
# |   6| 5004|
# |   7| 6048|
# |   8| 6305|
# |   9| 6128|
# |  10| 4745|
# |  11| 5391|
# |  12| 5996|
# |  13| 7075|
# |  14| 8446|
# |  15| 8934|
# |  16| 8606|
# |  17| 7618|
# |  18| 7903|
# |  19| 7957|
# |  20| 6254|
# |  21| 7573|
# |  22| 6768|
# |  23| 4928|
# +----+-----+

# from this we can say from 1 pm till 10 pm are the peak hours 
# in this time servers are having more traffics
# so it should be seen and actions should be taken accordingly


#version popularity
df_combined.groupBy("version").count().orderBy("count", ascending=False).show()
# +------------+------+                                                           
# |     version| count|
# +------------+------+
# |       4.5.2|129887|
# |       devel| 10878|
# |       4.4.3|  2809|

#from top 3 we can get the result
# the version 4.5.2  is mostly used by users
# working on the bugs in version 4.5.2 will be really help company and make customers happy 


#done a lot analysis about the data from 10 jan to 14 jan
#lets write the data partioned output by the column date and country both


#reading the output data

# df_combined.write.mode("overwrite").partitionBy("date", "country").parquet("/home/ayush/pyspark-workspace/project/output/download_logs")

# df_read = spark.read.parquet(
#     "/home/ayush/pyspark-workspace/output/download_logs"
# )

# # Spark reads ONLY this partition
# df_read.filter("date = '2026-01-10' AND country = 'US'").show()
